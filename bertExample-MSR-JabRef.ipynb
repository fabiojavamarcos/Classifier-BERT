{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Binary File: /Users/fd252/OneDrive/Research4/MSRExt/OSSPRMapper/outputs/jabref50/jabref50_binaryBodyTitle.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import string\n",
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "from os import path\n",
    "# project\n",
    "proj_name       = \"jabref50\"\n",
    "working_dir     = \"/Users/fd252/OneDrive/Research4/MSRExt/\"\n",
    "\n",
    "\n",
    "# input\n",
    "binaryBodyTitle = working_dir + \"OSSPRMapper/outputs/\" + proj_name + '/' + proj_name + '_' + \"binaryBodyTitle.csv\"\n",
    "print( \"Input Binary File: \" + binaryBodyTitle )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organizing data_frame to issue order\n",
    "def organize():\n",
    "    data_classes = pd.read_csv( binaryBodyTitle, header = 0, sep=';' )\n",
    "\n",
    "    # OBS!!!!!\n",
    "    #line 62 mockito was deleted wrong number of columns!\n",
    "    #line 1923 rxjava was deleted wrong number of columns!\n",
    "    \n",
    "    \n",
    "    ## Code implemented below before modifications by Jacob ##\n",
    "    # del data_classes['prIssue']\n",
    "    # del data_classes['issueTitle']\n",
    "    # del data_classes['issueBody']\n",
    "    \n",
    "    # these renames are listed in the order that the original name appears in the list of headers\n",
    "    # coming in from the binary file\n",
    "    # data_classes.rename(columns={ 'pr': 'prNumber',                                   \n",
    "    #                               'Title': 'prTitle',                                 \n",
    "    #                               'Body': 'prBody',                                   \n",
    "    #                               'issue': 'issueNumber',                             \n",
    "    #                               'issueComments': 'prComments',                      \n",
    "    #                               'issueTitleLink': 'issueTitle',                     \n",
    "    #                               'issueBodyLink': 'issueBody',                       \n",
    "    #                               'issueCommentsLink': 'issue_Comments',              \n",
    "    #                               'Comments': 'prCodeReviewComments' }, inplace=True) \n",
    "    \n",
    "    # categories = data_classes.columns.values.tolist()\n",
    "    \n",
    "    \n",
    "    # this list must match the headers coming in from the binary file in name and order, including\n",
    "    # with the renames above\n",
    "    # data_classes = data_classes[[ 'pr', 'Util', 'NLP', 'APM', 'Network', 'DB', 'Interpreter', 'Logging', \n",
    "    #                               'Thread', 'DataStructure', 'i18n', 'DevOps', 'Logic',  \n",
    "    #                               'Microservices', 'ML', 'Test', 'Search', 'IO', 'UI', 'Parser', 'Security',\n",
    "    #                               'Cloud', 'BigData', 'App', 'GIS', 'Title', 'Body', 'prIssue', 'issue', \n",
    "    #                               'issueTitle', 'issueBody', 'issueComments', 'issueTitleLink', 'issueBodyLink', \n",
    "    #                               'issueCommentsLink', 'isPR', 'isTrain', 'commitMessage', 'Comments']]  \n",
    "\n",
    "\n",
    "    ## Code above after modifications by Jacob ##\n",
    "    # these renames are listed in the order that the original name appears in the list of headers\n",
    "    # coming in from the binary file\n",
    "    data_classes.rename(columns={ 'pr': 'prNumber',                                   \n",
    "                                  'Title': 'prTitle',                                 \n",
    "                                  'Body': 'prBody',                                   \n",
    "                                  'issue': 'issueNumber',                             \n",
    "                                  'issueComments': 'prComments',                      \n",
    "                                  #'issueTitleLink': 'issueTitle',                     \n",
    "                                  #'issueBodyLink': 'issueBody',                       \n",
    "                                  'issueCommentsLink': 'issue_Comments',              \n",
    "                                  'Comments': 'prCodeReviewComments',\n",
    "                                'Data Structure': 'DataStructure',\n",
    "                                'Big Data': 'BigData'}, inplace=True) \n",
    "    \n",
    "    categories = data_classes.columns.values.tolist()\n",
    "    \n",
    "    \n",
    "    # this list must match the headers coming in from the binary file in name and order, including\n",
    "    # with the renames above\n",
    "    data_classes = data_classes[[ 'prNumber', 'Util', 'NLP', 'APM', 'Network', 'DB', 'Interpreter', 'Logging', \n",
    "                                  'Thread', 'DataStructure', 'i18n', 'DevOps', 'Logic',\n",
    "                                  'Microservices', 'ML', 'Test', 'Search', 'IO', 'UI', 'Parser', 'Security',\n",
    "                                  'Cloud', 'BigData', 'App', 'GIS', 'prTitle', 'prBody', 'prIssue', 'issueNumber', \n",
    "                                  'issueTitle', 'issueBody', 'prComments', 'issueTitleLink', 'issueBodyLink', \n",
    "                                  'issue_Comments', 'isPR', 'isTrain', 'commitMessage', 'prCodeReviewComments']]    \n",
    "\n",
    "    \n",
    "    data_classes['issueNumber'] = data_classes['issueNumber'].astype('Int64')\n",
    "    print('before filtering out empty classes',data_classes.shape)\n",
    "    \n",
    "    #find rows with parse error\n",
    "    data_classes_error = data_classes.loc[pd.isnull(data_classes.loc[:,'Util'])]\n",
    "    print('rows filtered out empty classes (parse error)',data_classes_error.shape)\n",
    "    \n",
    "    col_data_classes = len(data_classes.columns)\n",
    "\n",
    "    if (len(data_classes_error) > 0):\n",
    "        data_classes_fixed= data_classes_error.iloc[:,0].str.split(';', expand=True)\n",
    "        print('rows fixed after new parse - empty classes (parse error)',data_classes_fixed.shape)\n",
    "\n",
    "        col_data_classes_fixed = len(data_classes_fixed.columns)\n",
    "        \n",
    "        #removing rows with problems \n",
    "        data_classes.dropna(subset = [\"Util\"], inplace=True)\n",
    "        print('after filtering out empty classes',data_classes.shape)\n",
    "        \n",
    "        print('len columns data_classes:',col_data_classes)\n",
    "        print('len columns data_classes_fixed:',col_data_classes_fixed)\n",
    "        \n",
    "        if (col_data_classes == col_data_classes_fixed):\n",
    "            \n",
    "            names =['prNumber','DB','Interpreter','Logging','Thread','DataStructure','DevOps','i18n','Logic',\n",
    "                    'Microservices','ML','Test','Search','IO','UI','Parser','Security','Cloud','BigData','App',\n",
    "                    'GIS','Util','NLP','APM','Network','prTitle','prBody','prIssue','issueNumber','issueTitle',\n",
    "                    'issueBody','prComments','issueTitleLink','issueBodyLink','issue_Comments','isPR','isTrain',\n",
    "                    'commitMessage','prCodeReviewComments']                     \n",
    "            data_classes_fixed.columns = names\n",
    "\n",
    "            #drop data with error after parsing\n",
    "            index_names = data_classes_fixed[ (data_classes_fixed['Util'] != '0') & (data_classes_fixed['Util'] != '1') |\n",
    "                                     (data_classes_fixed['NLP'] != '0') & (data_classes_fixed['NLP'] != '1') |\n",
    "                                     (data_classes_fixed['APM'] != '0') & (data_classes_fixed['APM'] != '1') |\n",
    "                                     (data_classes_fixed['Network'] != '0') & (data_classes_fixed['Network'] != '1') |\n",
    "                                     (data_classes_fixed['DB'] != '0') & (data_classes_fixed['DB'] != '1') |\n",
    "                                     (data_classes_fixed['Interpreter'] != '0') & (data_classes_fixed['Interpreter'] != '1') |\n",
    "                                     (data_classes_fixed['Logging'] != '0') & (data_classes_fixed['Logging'] != '1') |\n",
    "                                     (data_classes_fixed['Thread'] != '0') & (data_classes_fixed['Thread'] != '1') |\n",
    "                                     (data_classes_fixed['DataStructure'] != '0') & (data_classes_fixed['DataStructure'] != '1') |\n",
    "                                     (data_classes_fixed['i18n'] != '0') & (data_classes_fixed['i18n'] != '1') |\n",
    "\n",
    "                                     (data_classes_fixed['DevOps'] != '0') & (data_classes_fixed['DevOps'] != '1') |\n",
    "                                     (data_classes_fixed['Logic'] != '0') & (data_classes_fixed['Logic'] != '1') |\n",
    "                                   (data_classes_fixed['Microservices'] != '0') & (data_classes_fixed['Microservices'] != '1') |\n",
    "                                   (data_classes_fixed['ML'] != '0') & (data_classes_fixed['ML'] != '1') |\n",
    "                                   (data_classes_fixed['Test'] != '0') & (data_classes_fixed['Test'] != '1') |\n",
    "                                   (data_classes_fixed['Search'] != '0') & (data_classes_fixed['Search'] != '1') |\n",
    "                                   (data_classes_fixed['IO'] != '0') & (data_classes_fixed['IO'] != '1') |\n",
    "                                   (data_classes_fixed['UI'] != '0') & (data_classes_fixed['UI'] != '1') |\n",
    "                                   (data_classes_fixed['Parser'] != '0') & (data_classes_fixed['Parser'] != '1') |\n",
    "                                 (data_classes_fixed['Security'] != '0') & (data_classes_fixed['Security'] != '1') |\n",
    "                                 (data_classes_fixed['Cloud'] != '0') & (data_classes_fixed['Cloud'] != '1') |\n",
    "                                 (data_classes_fixed['BigData'] != '0') & (data_classes_fixed['BigData'] != '1') |\n",
    "                                 (data_classes_fixed['App'] != '0') & (data_classes_fixed['App'] != '1') |\n",
    "                                 (data_classes_fixed['GIS'] != '0') & (data_classes_fixed['GIS'] != '1') \n",
    "\n",
    "                                    ].index\n",
    "\n",
    "            # drop these given row\n",
    "            # indexes from dataFrame\n",
    "            data_classes_fixed.drop(index_names, inplace = True)\n",
    "            print('data fixed after dropping parse fix errors',data_classes_fixed.shape)\n",
    "\n",
    "            #back to float\n",
    "            data_classes_fixed['Util'] = data_classes_fixed['Util'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['NLP'] = data_classes_fixed['NLP'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['APM'] = data_classes_fixed['APM'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Network'] = data_classes_fixed['Network'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['DB'] = data_classes_fixed['DB'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Interpreter'] = data_classes_fixed['Interpreter'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Logging'] = data_classes_fixed['Logging'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Thread'] = data_classes_fixed['Thread'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['DataStructure'] = data_classes_fixed['DataStructure'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['i18n'] = data_classes_fixed['i18n'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['DevOps'] = data_classes_fixed['DevOps'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Logic'] = data_classes_fixed['Logic'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Microservices'] = data_classes_fixed['Microservices'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['ML'] = data_classes_fixed['ML'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Test'] = data_classes_fixed['Test'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Search'] = data_classes_fixed['Search'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['IO'] = data_classes_fixed['IO'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['UI'] = data_classes_fixed['UI'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Parser'] = data_classes_fixed['Parser'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Security'] = data_classes_fixed['Security'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Cloud'] = data_classes_fixed['Cloud'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['BigData'] = data_classes_fixed['BigData'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['App'] = data_classes_fixed['App'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['GIS'] = data_classes_fixed['GIS'].astype(str).astype('Float64')\n",
    "\n",
    "            # appending fixed rows\n",
    "            data_classes_new = data_classes.append(data_classes_fixed)\n",
    "            print('after appending fixed rows',data_classes_new.shape)\n",
    "\n",
    "            return data_classes_new\n",
    "        \n",
    "        else:\n",
    "            print('fixing parse errors failed')\n",
    "\n",
    "            return data_classes\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('no parse errors found')\n",
    "        return data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering issues with PRs\n",
    "def filtering(data_classes):\n",
    "    \n",
    "    print('before filtering out isTrain == 0',data_classes.shape)\n",
    "\n",
    "    IssuePRDataset = data_classes[data_classes[\"isTrain\"] == 0]\n",
    "    \n",
    "    print('after filtering out isTrain == 0',IssuePRDataset.shape)\n",
    "\n",
    "\n",
    "    #invalid number of issue = NaN\n",
    "    # IssuePRDataset = IssuePRDataset.drop([1805])\n",
    "\n",
    "    categories = IssuePRDataset.columns.values.tolist()\n",
    "    \n",
    "    return categories, IssuePRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1.a - o quão sensível o resultado é em relação ao algoritmo? \n",
    "#vários algoritmos - BinaryRelevance\n",
    "#todas as palavras, bootstrap, unigram \n",
    "#somente o título\n",
    "def dataset_config(IssuePRDataset):\n",
    "    # ORIGINAL\n",
    "    # data_test1 = IssuePRDataset[['issueNumber','prNumber','issueTitle','Google Common', \n",
    "    #                              'Test', 'SO', 'IO', 'UI', 'Network', 'Security', \n",
    "    #                              'OpenOffice Documents', 'Database', 'Utils', 'PDF', \n",
    "    #                              'Logging', 'Latex']].copy()\n",
    "    \n",
    "    # WORKS WITH NEW INPUTS\n",
    "    # data_test1 = IssuePRDataset[['issueNumber','prNumber','issueTitle', 'Test','IO', 'UI', 'Network', 'Security', 'Logging' ]].copy() \n",
    "\n",
    "    data_test1 = IssuePRDataset[[ 'issueNumber','prNumber','issueTitle','issueBody', 'prTitle', 'prBody',\n",
    "                             'issueTitleLink','issueBodyLink','commitMessage','prComments',\n",
    "                             'Util', \n",
    "                                  'NLP', 'APM', 'Network', 'DB', 'Interpreter',\n",
    "                                  'Logging', 'Thread', 'DataStructure', 'i18n', \n",
    "                                  'DevOps', 'Logic', 'Microservices', 'ML',\n",
    "                                  'Test', 'Search', 'IO', 'UI', 'Parser', 'Security',\n",
    "                                  'Cloud', 'BigData', 'App', 'GIS' ]].copy()\n",
    "    #print(type(data_test1))\n",
    "    #data_test1['corpus'] = IssuePRDataset['issueTitle'] + IssuePRDataset['issueBody']\n",
    "    data_test1[\"corpus\"] = data_test1[\"issueTitle\"].map(str)+\" \"+ data_test1[\"issueBody\"].map(str)+\" \"+ data_test1[\"prComments\"].map(str)\n",
    "\n",
    "    # rxjava 2489 terms\n",
    "    # mockito 598\n",
    "    # presto 4\n",
    "    # guava 1140\n",
    "    # jabref 740\n",
    "    \n",
    "    #data_test1[\"corpus\"] = data_test1[\"issueTitle\"].map(str) + ' ' + data_test1[\"issueBody\"].map(str) + ' ' + data_test1[\"prTitle\"].map(str) + ' ' + data_test1[\"prBody\"].map(str)\n",
    "    # rxjava 3002 terms\n",
    "    \n",
    "    del data_test1[\"issueTitle\"]\n",
    "    del data_test1[\"issueBody\"]\n",
    "    del data_test1[\"prTitle\"]\n",
    "    del data_test1[\"prBody\"]\n",
    "    del data_test1[\"issueTitleLink\"]\n",
    "    del data_test1[\"issueBodyLink\"]\n",
    "    del data_test1[\"commitMessage\"]\n",
    "    del data_test1[\"prComments\"]\n",
    "\n",
    "    print('before filtering out empty corpus',data_test1.shape)\n",
    "    data_test1.dropna(subset = [\"corpus\"], inplace=True)\n",
    "    \n",
    "    data_test1['corpus'] = data_test1['corpus'].str.replace(\"nan\",' ')\n",
    "    print('after filtering out empty corpus',data_test1.shape)\n",
    "\n",
    "    #removing utils because we won't to predict a so simple API that is basically used in all PRs\n",
    "    #del data_test1[\"Util\"]\n",
    "\n",
    "    data_test1 = data_test1.reset_index(drop=True)\n",
    "    \n",
    "    return data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering out empty classes (1840, 39)\n",
      "rows filtered out empty classes (parse error) (0, 39)\n",
      "no parse errors found\n"
     ]
    }
   ],
   "source": [
    "data_classes = organize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering out isTrain == 0 (1840, 39)\n",
      "after filtering out isTrain == 0 (716, 39)\n"
     ]
    }
   ],
   "source": [
    "categories, IssuePRDataset = filtering(data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text\n",
    "\n",
    "#We first convert the comments to lower-case \n",
    "#then use custom made functions to remove html-tags, punctuation and non-alphabetic characters from the TitleBody.\n",
    "\n",
    "def clean_data(data_test1):\n",
    "    if not sys.warnoptions:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def cleanHtml(sentence):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "        return cleantext\n",
    "\n",
    "    def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "        cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "        cleaned = cleaned.strip()\n",
    "        cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "        return cleaned\n",
    "\n",
    "    def keepAlpha(sentence):\n",
    "        alpha_sent = \"\"\n",
    "        for word in sentence.split():\n",
    "            alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "            alpha_sent += alpha_word\n",
    "            alpha_sent += \" \"\n",
    "        alpha_sent = alpha_sent.strip()\n",
    "        return alpha_sent\n",
    "\n",
    "    #function pra remover palavras com menos de 3 tokens\n",
    "\n",
    "    data_test1['corpus'] = data_test1['corpus'].str.lower()\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(cleanHtml)\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(cleanPunc)\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(keepAlpha)\n",
    "    \n",
    "    return data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### removing stopwords\n",
    "\n",
    "def remove_stop_words():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['nan','pr','zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within','jabref','org','github','com','md','https','ad','changelog','','joelparkerhenderson','localizationupd',' localizationupd','localizationupd ','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the','Mr', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'])\n",
    "    #stop_words.update(['i', 'me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\"Mr\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "    re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "\n",
    "    return re_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(sentence, re_stop_words):\n",
    "    #global re_stop_words\n",
    "    #print(sentence)\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "#removing words with less than 3 characters\n",
    "#data_classes['titleBody'] = data_classes['titleBody'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stem(data_test1):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    def stemming(sentence):\n",
    "        stemSentence = \"\"\n",
    "        for word in sentence.split():\n",
    "            stem = stemmer.stem(word)\n",
    "            stemSentence += stem\n",
    "            stemSentence += \" \"\n",
    "        stemSentence = stemSentence.strip()\n",
    "        return stemSentence\n",
    "    \n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(stemming)\n",
    "    #print(data_test1['corpus'])\n",
    "    \n",
    "    return data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering out empty corpus (716, 27)\n",
      "after filtering out empty corpus (716, 27)\n"
     ]
    }
   ],
   "source": [
    "data_test1 = dataset_config(IssuePRDataset)\n",
    "data_test1 = clean_data(data_test1)\n",
    "#print('################# data_test1 after fixing')\n",
    "#print(data_test1)\n",
    "\n",
    "#   print('1',data_test1['corpus'])\n",
    "\n",
    "re_stop_words = remove_stop_words()\n",
    "data_test1['corpus'] = data_test1['corpus'].apply(removeStopWords, re_stop_words=re_stop_words)\n",
    "data = data_test1\n",
    "#   print('2',data_test1['corpus'])\n",
    "data_test1 = apply_stem(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test1.to_csv('./data/binaryNew.csv', encoding='utf-8', header=True, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Util',\n",
       " 'NLP',\n",
       " 'APM',\n",
       " 'Network',\n",
       " 'DB',\n",
       " 'Interpreter',\n",
       " 'Logging',\n",
       " 'Thread',\n",
       " 'DataStructure',\n",
       " 'i18n',\n",
       " 'DevOps',\n",
       " 'Logic',\n",
       " 'Microservices',\n",
       " 'ML',\n",
       " 'Test',\n",
       " 'Search',\n",
       " 'IO',\n",
       " 'UI',\n",
       " 'Parser',\n",
       " 'Security',\n",
       " 'Cloud',\n",
       " 'BigData',\n",
       " 'App',\n",
       " 'GIS']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data.columns.values.tolist()\n",
    "labels = cols[2:len(cols)-1]\n",
    "labelsdf = pd.DataFrame(labels)\n",
    "labelsdf.to_csv('./data/labelsNew.csv', encoding='utf-8', header=False, index=False, sep=',')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "DATA_PATH = './data/'\n",
    "LABEL_PATH = './data/'\n",
    "from fast_bert.data_cls import BertDataBunch\n",
    "\n",
    "databunch = BertDataBunch(DATA_PATH, LABEL_PATH,\n",
    "                          tokenizer='bert-base-uncased',\n",
    "                          #train_file='train.csv',\n",
    "                          #val_file='val.csv',\n",
    "                          #label_file='labels.csv',\n",
    "                          train_file='binaryNew.csv',\n",
    "                          val_file='binaryNew.csv',\n",
    "                          label_file='labelsNew.csv',\n",
    "                          #text_col='text',\n",
    "                          text_col='corpus',\n",
    "                          label_col=labels,\n",
    "                          batch_size_per_gpu=16,\n",
    "                          max_seq_length=512,\n",
    "                          multi_gpu=False,\n",
    "                          multi_label=True,\n",
    "                          model_type='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import accuracy\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "#device_cuda = torch.device(\"cuda\")\n",
    "metrics = [{'name': 'accuracy', 'function': accuracy}]\n",
    "\n",
    "OUTPUT_DIR = './output/'\n",
    "\n",
    "learner = BertLearner.from_pretrained_model(\n",
    "\t\t\t\t\t\tdatabunch,\n",
    "\t\t\t\t\t\tpretrained_path='bert-base-uncased',\n",
    "\t\t\t\t\t\tmetrics=metrics,\n",
    "\t\t\t\t\t\t#device=device_cuda,\n",
    "                        device=None,\n",
    "\t\t\t\t\t\tlogger=logger,\n",
    "\t\t\t\t\t\toutput_dir=OUTPUT_DIR,\n",
    "\t\t\t\t\t\tfinetuned_wgts_path=None,\n",
    "\t\t\t\t\t\twarmup_steps=500,\n",
    "\t\t\t\t\t\tmulti_gpu=False,\n",
    "\t\t\t\t\t\tis_fp16=True,\n",
    "\t\t\t\t\t\tmulti_label=True,\n",
    "\t\t\t\t\t\tlogging_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100186401b224d1bac230d455bd930cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64, 6])) must be the same as input size (torch.Size([16, 6]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-fc2a900ad6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lamb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/fast_bert/learner_cls.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, use_val_loss, optimizer_type, num_iter, step_mode, smooth_f, diverge_th)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# train on batch and retrieve loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_val_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/fast_bert/learner_cls.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, train_iter)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/fast_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             loss = loss_fct(\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    714\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([64, 6])) must be the same as input size (torch.Size([16, 6]))"
     ]
    }
   ],
   "source": [
    "learner.lr_find(start_lr=1e-5,optimizer_type='lamb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/6 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/45 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64, 6])) must be the same as input size (torch.Size([16, 6]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6fc870eefb7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m learner.fit(epochs=6,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Evaluate the model after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mschedule_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"warmup_cosine\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \t\t\toptimizer_type=\"lamb\")\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/fast_bert/learner_cls.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, validate, return_results, schedule_type, optimizer_type)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# Run training step and get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/fast_bert/learner_cls.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/fast_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             loss = loss_fct(\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    714\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([64, 6])) must be the same as input size (torch.Size([16, 6]))"
     ]
    }
   ],
   "source": [
    "learner.fit(epochs=6,\n",
    "\t\t\tlr=6e-5,\n",
    "\t\t\tvalidate=True, \t# Evaluate the model after each epoch\n",
    "\t\t\tschedule_type=\"warmup_cosine\",\n",
    "\t\t\toptimizer_type=\"lamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
