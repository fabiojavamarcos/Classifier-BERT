{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertForMultiLabelSequenceClassification(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertForMultiLabelSequenceClassification(\n",
    "  (bert): BertModel(\n",
    "    (embeddings): BertEmbeddings(\n",
    "      (word_embeddings): Embedding(28996, 768)\n",
    "      (position_embeddings): Embedding(512, 768)\n",
    "      (token_type_embeddings): Embedding(2, 768)\n",
    "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
    "      (dropout): Dropout(p=0.1)\n",
    "    )\n",
    "    (encoder): BertEncoder(\n",
    "      (layer): ModuleList(\n",
    "#       12 BertLayers\n",
    "        (11): BertLayer(\n",
    "          (attention): BertAttention(\n",
    "            (self): BertSelfAttention(\n",
    "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (dropout): Dropout(p=0.1)\n",
    "            )\n",
    "            (output): BertSelfOutput(\n",
    "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1)\n",
    "            )\n",
    "          )\n",
    "          (intermediate): BertIntermediate(\n",
    "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "          )\n",
    "          (output): BertOutput(\n",
    "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (pooler): BertPooler(\n",
    "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (activation): Tanh()\n",
    "    )\n",
    "  )\n",
    "  (dropout): Dropout(p=0.1)\n",
    "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_thresh(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "\n",
    "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_labels):\n",
    "    fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Binary File: /Users/fd252/OneDrive/Research4/MSRExt/OSSPRMapper/outputs/rxjava_2/rxjava_2_binaryBodyTitle.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import string\n",
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "from os import path\n",
    "# project\n",
    "proj_name       = \"rxjava_2\"\n",
    "working_dir     = \"/Users/fd252/OneDrive/Research4/MSRExt/\"\n",
    "\n",
    "\n",
    "# input\n",
    "binaryBodyTitle = working_dir + \"OSSPRMapper/outputs/\" + proj_name + '/' + proj_name + '_' + \"binaryBodyTitle.csv\"\n",
    "print( \"Input Binary File: \" + binaryBodyTitle )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organizing data_frame to issue order\n",
    "def organize():\n",
    "    try:\n",
    "        #data = pd.read_csv('file1.csv',skiprows=line)\n",
    "        data_classes = pd.read_csv( binaryBodyTitle, header = 0, sep=';')\n",
    "    except:\n",
    "        print ('number of columns varying. Skipping bad lines!')\n",
    "        data_classes = pd.read_csv(binaryBodyTitle, header = 0, sep=';', error_bad_lines=False)\n",
    "\n",
    "    # OBS!!!!!\n",
    "    #line 62 mockito was deleted wrong number of columns!\n",
    "    #line 1923 rxjava was deleted wrong number of columns!\n",
    "    \n",
    "    \n",
    "    ## Code implemented below before modifications by Jacob ##\n",
    "    # del data_classes['prIssue']\n",
    "    # del data_classes['issueTitle']\n",
    "    # del data_classes['issueBody']\n",
    "    \n",
    "    # these renames are listed in the order that the original name appears in the list of headers\n",
    "    # coming in from the binary file\n",
    "    # data_classes.rename(columns={ 'pr': 'prNumber',                                   \n",
    "    #                               'Title': 'prTitle',                                 \n",
    "    #                               'Body': 'prBody',                                   \n",
    "    #                               'issue': 'issueNumber',                             \n",
    "    #                               'issueComments': 'prComments',                      \n",
    "    #                               'issueTitleLink': 'issueTitle',                     \n",
    "    #                               'issueBodyLink': 'issueBody',                       \n",
    "    #                               'issueCommentsLink': 'issue_Comments',              \n",
    "    #                               'Comments': 'prCodeReviewComments' }, inplace=True) \n",
    "        \n",
    "    \n",
    "    # this list must match the headers coming in from the binary file in name and order, including\n",
    "    # with the renames above\n",
    "    # data_classes = data_classes[[ 'pr', 'Util', 'NLP', 'APM', 'Network', 'DB', 'Interpreter', 'Logging', \n",
    "    #                               'Thread', 'DataStructure', 'i18n', 'DevOps', 'Logic',  \n",
    "    #                               'Microservices', 'ML', 'Test', 'Search', 'IO', 'UI', 'Parser', 'Security',\n",
    "    #                               'Cloud', 'BigData', 'App', 'GIS', 'Title', 'Body', 'prIssue', 'issue', \n",
    "    #                               'issueTitle', 'issueBody', 'issueComments', 'issueTitleLink', 'issueBodyLink', \n",
    "    #                               'issueCommentsLink', 'isPR', 'isTrain', 'commitMessage', 'Comments']]  \n",
    "\n",
    "\n",
    "    ## Code above after modifications by Jacob ##\n",
    "    # these renames are listed in the order that the original name appears in the list of headers\n",
    "    # coming in from the binary file\n",
    "    data_classes.rename(columns={ 'pr': 'prNumber',                                   \n",
    "                                  'Title': 'prTitle',                                 \n",
    "                                  'Body': 'prBody',                                   \n",
    "                                  'issue': 'issueNumber',                             \n",
    "                                  'issueComments': 'prComments',                      \n",
    "                                  #'issueTitleLink': 'issueTitle',                     \n",
    "                                  #'issueBodyLink': 'issueBody',                       \n",
    "                                  'issueCommentsLink': 'issue_Comments',              \n",
    "                                  'Comments': 'prCodeReviewComments',\n",
    "                                #'Data Structure': 'DataStructure',\n",
    "                                #'Big Data': 'BigData'\n",
    "                                }, inplace=True) \n",
    "    \n",
    "    categories = data_classes.columns.values.tolist()\n",
    "    print('categories:',categories)    \n",
    "    \n",
    "    # this list must match the headers coming in from the binary file in name and order, including\n",
    "    # with the renames above\n",
    "    #data_classes = data_classes[[ 'prNumber', 'Util', 'NLP', 'APM', 'Network', 'DB', 'Interpreter', 'Logging', \n",
    "    #                              'Thread', 'DataStructure', 'i18n', 'DevOps', 'Logic',\n",
    "    #                              'Microservices', 'ML', 'Test', 'Search', 'IO', 'UI', 'Parser', 'Security',\n",
    "    #                              'Cloud', 'BigData', 'App', 'GIS', 'prTitle', 'prBody', 'prIssue', 'issueNumber', \n",
    "    #                              'issueTitle', 'issueBody', 'prComments', 'issueTitleLink', 'issueBodyLink', \n",
    "    #                              'issue_Comments', 'isPR', 'isTrain', 'commitMessage', 'prCodeReviewComments']]    \n",
    "\n",
    "    data_classes = data_classes[ categories]    \n",
    "\n",
    "    \n",
    "    data_classes['issueNumber'] = data_classes['issueNumber'].astype('Int64')\n",
    "    print('before filtering out empty classes',data_classes.shape)\n",
    "    \n",
    "    #find rows with parse error\n",
    "    data_classes_error = data_classes.loc[pd.isnull(data_classes.loc[:,'Util'])]\n",
    "    print('rows filtered out empty classes (parse error)',data_classes_error.shape)\n",
    "    \n",
    "    col_data_classes = len(data_classes.columns)\n",
    "\n",
    "    if (len(data_classes_error) > 0):\n",
    "        data_classes_fixed= data_classes_error.iloc[:,0].str.split(';', expand=True)\n",
    "        print('rows fixed after new parse - empty classes (parse error)',data_classes_fixed.shape)\n",
    "\n",
    "        col_data_classes_fixed = len(data_classes_fixed.columns)\n",
    "        \n",
    "        #removing rows with problems \n",
    "        data_classes.dropna(subset = [\"Util\"], inplace=True)\n",
    "        print('after filtering out empty classes',data_classes.shape)\n",
    "        \n",
    "        print('len columns data_classes:',col_data_classes)\n",
    "        print('len columns data_classes_fixed:',col_data_classes_fixed)\n",
    "        \n",
    "        if (col_data_classes == col_data_classes_fixed):\n",
    "            \n",
    "            names =['prNumber','DB','Interpreter','Logging','Thread','DataStructure','DevOps','i18n','Logic',\n",
    "                    'Microservices','ML','Test','Search','IO','UI','Parser','Security','Cloud','BigData','App',\n",
    "                    'GIS','Util','NLP','APM','Network','prTitle','prBody','prIssue','issueNumber','issueTitle',\n",
    "                    'issueBody','prComments','issueTitleLink','issueBodyLink','issue_Comments','isPR','isTrain',\n",
    "                    'commitMessage','prCodeReviewComments']                     \n",
    "            data_classes_fixed.columns = names\n",
    "\n",
    "            #drop data with error after parsing\n",
    "            index_names = data_classes_fixed[ (data_classes_fixed['Util'] != '0') & (data_classes_fixed['Util'] != '1') |\n",
    "                                     (data_classes_fixed['NLP'] != '0') & (data_classes_fixed['NLP'] != '1') |\n",
    "                                     (data_classes_fixed['APM'] != '0') & (data_classes_fixed['APM'] != '1') |\n",
    "                                     (data_classes_fixed['Network'] != '0') & (data_classes_fixed['Network'] != '1') |\n",
    "                                     (data_classes_fixed['DB'] != '0') & (data_classes_fixed['DB'] != '1') |\n",
    "                                     (data_classes_fixed['Interpreter'] != '0') & (data_classes_fixed['Interpreter'] != '1') |\n",
    "                                     (data_classes_fixed['Logging'] != '0') & (data_classes_fixed['Logging'] != '1') |\n",
    "                                     (data_classes_fixed['Thread'] != '0') & (data_classes_fixed['Thread'] != '1') |\n",
    "                                     (data_classes_fixed['DataStructure'] != '0') & (data_classes_fixed['DataStructure'] != '1') |\n",
    "                                     (data_classes_fixed['i18n'] != '0') & (data_classes_fixed['i18n'] != '1') |\n",
    "\n",
    "                                     (data_classes_fixed['DevOps'] != '0') & (data_classes_fixed['DevOps'] != '1') |\n",
    "                                     (data_classes_fixed['Logic'] != '0') & (data_classes_fixed['Logic'] != '1') |\n",
    "                                   (data_classes_fixed['Microservices'] != '0') & (data_classes_fixed['Microservices'] != '1') |\n",
    "                                   (data_classes_fixed['ML'] != '0') & (data_classes_fixed['ML'] != '1') |\n",
    "                                   (data_classes_fixed['Test'] != '0') & (data_classes_fixed['Test'] != '1') |\n",
    "                                   (data_classes_fixed['Search'] != '0') & (data_classes_fixed['Search'] != '1') |\n",
    "                                   (data_classes_fixed['IO'] != '0') & (data_classes_fixed['IO'] != '1') |\n",
    "                                   (data_classes_fixed['UI'] != '0') & (data_classes_fixed['UI'] != '1') |\n",
    "                                   (data_classes_fixed['Parser'] != '0') & (data_classes_fixed['Parser'] != '1') |\n",
    "                                 (data_classes_fixed['Security'] != '0') & (data_classes_fixed['Security'] != '1') |\n",
    "                                 (data_classes_fixed['Cloud'] != '0') & (data_classes_fixed['Cloud'] != '1') |\n",
    "                                 (data_classes_fixed['BigData'] != '0') & (data_classes_fixed['BigData'] != '1') |\n",
    "                                 (data_classes_fixed['App'] != '0') & (data_classes_fixed['App'] != '1') |\n",
    "                                 (data_classes_fixed['GIS'] != '0') & (data_classes_fixed['GIS'] != '1') |\n",
    "                                     (data_classes_fixed['Error Handling'] != '0') & (data_classes_fixed['Error Handling'] != '1') |\n",
    "                                     (data_classes_fixed['Event Handling'] != '0') & (data_classes_fixed['Event Handling'] != '1') |\n",
    "                                     (data_classes_fixed['Lang'] != '0') & (data_classes_fixed['Lang'] != '1') |\n",
    "                                     (data_classes_fixed['Setup'] != '0') & (data_classes_fixed['Setup'] != '1')\n",
    "                                    ].index\n",
    "\n",
    "            # drop these given row\n",
    "            # indexes from dataFrame\n",
    "            data_classes_fixed.drop(index_names, inplace = True)\n",
    "            print('data fixed after dropping parse fix errors',data_classes_fixed.shape)\n",
    "\n",
    "            #back to float\n",
    "            data_classes_fixed['Util'] = data_classes_fixed['Util'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['NLP'] = data_classes_fixed['NLP'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['APM'] = data_classes_fixed['APM'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Network'] = data_classes_fixed['Network'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['DB'] = data_classes_fixed['DB'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Interpreter'] = data_classes_fixed['Interpreter'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Logging'] = data_classes_fixed['Logging'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Thread'] = data_classes_fixed['Thread'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['DataStructure'] = data_classes_fixed['DataStructure'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['i18n'] = data_classes_fixed['i18n'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['DevOps'] = data_classes_fixed['DevOps'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Logic'] = data_classes_fixed['Logic'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Microservices'] = data_classes_fixed['Microservices'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['ML'] = data_classes_fixed['ML'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Test'] = data_classes_fixed['Test'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Search'] = data_classes_fixed['Search'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['IO'] = data_classes_fixed['IO'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['UI'] = data_classes_fixed['UI'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Parser'] = data_classes_fixed['Parser'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Security'] = data_classes_fixed['Security'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Cloud'] = data_classes_fixed['Cloud'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['BigData'] = data_classes_fixed['BigData'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['App'] = data_classes_fixed['App'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['GIS'] = data_classes_fixed['GIS'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Error Handling'] = data_classes_fixed['Error Handling'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Event Handling'] = data_classes_fixed['Event Handling'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Lang'] = data_classes_fixed['Lang'].astype(str).astype('Float64')\n",
    "            data_classes_fixed['Setup'] = data_classes_fixed['Setup'].astype(str).astype('Float64')\n",
    "\n",
    "            # appending fixed rows\n",
    "            data_classes_new = data_classes.append(data_classes_fixed)\n",
    "            print('after appending fixed rows',data_classes_new.shape)\n",
    "\n",
    "            return data_classes_new\n",
    "        \n",
    "        else:\n",
    "            print('fixing parse errors failed')\n",
    "\n",
    "            return data_classes\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('no parse errors found')\n",
    "        return data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering issues with PRs\n",
    "def filtering(data_classes):\n",
    "    \n",
    "    print('before filtering out isTrain == 0',data_classes.shape)\n",
    "\n",
    "    IssuePRDataset = data_classes[data_classes[\"isTrain\"] == 0]\n",
    "    \n",
    "    print('after filtering out isTrain == 0',IssuePRDataset.shape)\n",
    "\n",
    "\n",
    "    #invalid number of issue = NaN\n",
    "    # IssuePRDataset = IssuePRDataset.drop([1805])\n",
    "\n",
    "    categories = IssuePRDataset.columns.values.tolist()\n",
    "    \n",
    "    return categories, IssuePRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1.a - o quão sensível o resultado é em relação ao algoritmo? \n",
    "#vários algoritmos - BinaryRelevance\n",
    "#todas as palavras, bootstrap, unigram \n",
    "#somente o título\n",
    "def dataset_config(IssuePRDataset):\n",
    "    # ORIGINAL\n",
    "    # data_test1 = IssuePRDataset[['issueNumber','prNumber','issueTitle','Google Common', \n",
    "    #                              'Test', 'SO', 'IO', 'UI', 'Network', 'Security', \n",
    "    #                              'OpenOffice Documents', 'Database', 'Utils', 'PDF', \n",
    "    #                              'Logging', 'Latex']].copy()\n",
    "    \n",
    "    # WORKS WITH NEW INPUTS\n",
    "    # data_test1 = IssuePRDataset[['issueNumber','prNumber','issueTitle', 'Test','IO', 'UI', 'Network', 'Security', 'Logging' ]].copy() \n",
    "\n",
    "    #data_test1 = IssuePRDataset[[ 'issueNumber','prNumber','issueTitle','issueBody', 'prTitle', 'prBody',\n",
    "    #                         'issueTitleLink','issueBodyLink','commitMessage','prComments',\n",
    "    #                         'Util', \n",
    "    #                              'NLP', 'APM', 'Network', 'DB', 'Interpreter',\n",
    "    #                              'Logging', 'Thread', 'DataStructure', 'i18n', \n",
    "    #                              'DevOps', 'Logic', 'Microservices', 'ML',\n",
    "    #                              'Test', 'Search', 'IO', 'UI', 'Parser', 'Security',\n",
    "    #                              'Cloud', 'BigData', 'App', 'GIS' ]].copy()\n",
    "    \n",
    "    data_test1 = IssuePRDataset[[ 'issueNumber','prNumber','issueTitle','issueBody', 'prTitle', 'prBody',\n",
    "                             'issueTitleLink','issueBodyLink','commitMessage','prComments',\n",
    "                             'Util','NLP','APM','Network','DB','Interpreter','Error Handling','Logging','Lang','Data Structure','DevOps','i18n','Setup','Logic','Microservices','ML','Test','Search','IO','UI','Parser','Security','Cloud','Big Data','Event Handling','App','GIS' ]].copy()\n",
    "\n",
    "    #print(type(data_test1))\n",
    "    #data_test1['corpus'] = IssuePRDataset['issueTitle'] + IssuePRDataset['issueBody']\n",
    "    data_test1[\"corpus\"] = data_test1[\"issueTitle\"].map(str)+\" \"+ data_test1[\"issueBody\"].map(str)+\" \"+ data_test1[\"prComments\"].map(str)\n",
    "\n",
    "    # rxjava 2489 terms\n",
    "    # mockito 598\n",
    "    # presto 4\n",
    "    # guava 1140\n",
    "    # jabref 740\n",
    "    \n",
    "    #data_test1[\"corpus\"] = data_test1[\"issueTitle\"].map(str) + ' ' + data_test1[\"issueBody\"].map(str) + ' ' + data_test1[\"prTitle\"].map(str) + ' ' + data_test1[\"prBody\"].map(str)\n",
    "    # rxjava 3002 terms\n",
    "    \n",
    "    del data_test1[\"issueTitle\"]\n",
    "    del data_test1[\"issueBody\"]\n",
    "    del data_test1[\"prTitle\"]\n",
    "    del data_test1[\"prBody\"]\n",
    "    del data_test1[\"issueTitleLink\"]\n",
    "    del data_test1[\"issueBodyLink\"]\n",
    "    del data_test1[\"commitMessage\"]\n",
    "    del data_test1[\"prComments\"]\n",
    "\n",
    "    print('before filtering out empty corpus',data_test1.shape)\n",
    "    data_test1.dropna(subset = [\"corpus\"], inplace=True)\n",
    "    \n",
    "    data_test1['corpus'] = data_test1['corpus'].str.replace(\"nan\",' ')\n",
    "    print('after filtering out empty corpus',data_test1.shape)\n",
    "\n",
    "    #removing utils because we won't to predict a so simple API that is basically used in all PRs\n",
    "    #del data_test1[\"Util\"]\n",
    "\n",
    "    data_test1 = data_test1.reset_index(drop=True)\n",
    "    \n",
    "    return data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns varying. Skipping bad lines!\n",
      "categories: ['prNumber', 'Util', 'NLP', 'APM', 'Network', 'DB', 'Interpreter', 'Error Handling', 'Logging', 'Lang', 'Data Structure', 'DevOps', 'i18n', 'Setup', 'Logic', 'Microservices', 'ML', 'Test', 'Search', 'IO', 'UI', 'Parser', 'Security', 'Cloud', 'Big Data', 'Event Handling', 'App', 'GIS', 'prTitle', 'prBody', 'prIssue', 'issueNumber', 'issueTitle', 'issueBody', 'prComments', 'issueTitleLink', 'issueBodyLink', 'issue_Comments', 'isPR', 'isTrain', 'commitMessage', 'prCodeReviewComments']\n",
      "before filtering out empty classes (1977, 42)\n",
      "rows filtered out empty classes (parse error) (0, 42)\n",
      "no parse errors found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1923: expected 42 fields, saw 45\\n'\n"
     ]
    }
   ],
   "source": [
    "data_classes = organize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering out isTrain == 0 (1977, 42)\n",
      "after filtering out isTrain == 0 (625, 42)\n"
     ]
    }
   ],
   "source": [
    "categories, IssuePRDataset = filtering(data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text\n",
    "\n",
    "#We first convert the comments to lower-case \n",
    "#then use custom made functions to remove html-tags, punctuation and non-alphabetic characters from the TitleBody.\n",
    "\n",
    "def clean_data(data_test1):\n",
    "    if not sys.warnoptions:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def cleanHtml(sentence):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "        return cleantext\n",
    "\n",
    "    def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "        cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "        cleaned = cleaned.strip()\n",
    "        cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "        return cleaned\n",
    "\n",
    "    def keepAlpha(sentence):\n",
    "        alpha_sent = \"\"\n",
    "        for word in sentence.split():\n",
    "            alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "            alpha_sent += alpha_word\n",
    "            alpha_sent += \" \"\n",
    "        alpha_sent = alpha_sent.strip()\n",
    "        return alpha_sent\n",
    "\n",
    "    #function pra remover palavras com menos de 3 tokens\n",
    "\n",
    "    data_test1['corpus'] = data_test1['corpus'].str.lower()\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(cleanHtml)\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(cleanPunc)\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(keepAlpha)\n",
    "    \n",
    "    return data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### removing stopwords\n",
    "\n",
    "def remove_stop_words():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['nan','pr','zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within','jabref','org','github','com','md','https','ad','changelog','','joelparkerhenderson','localizationupd',' localizationupd','localizationupd ','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the','Mr', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'])\n",
    "    #stop_words.update(['i', 'me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\"Mr\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "    re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "\n",
    "    return re_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(sentence, re_stop_words):\n",
    "    #global re_stop_words\n",
    "    #print(sentence)\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "#removing words with less than 3 characters\n",
    "#data_classes['titleBody'] = data_classes['titleBody'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stem(data_test1):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    def stemming(sentence):\n",
    "        stemSentence = \"\"\n",
    "        for word in sentence.split():\n",
    "            stem = stemmer.stem(word)\n",
    "            stemSentence += stem\n",
    "            stemSentence += \" \"\n",
    "        stemSentence = stemSentence.strip()\n",
    "        return stemSentence\n",
    "    \n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(stemming)\n",
    "    #print(data_test1['corpus'])\n",
    "    \n",
    "    return data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering out empty corpus (625, 30)\n",
      "after filtering out empty corpus (625, 30)\n"
     ]
    }
   ],
   "source": [
    "data_test1 = dataset_config(IssuePRDataset)\n",
    "data_test1 = clean_data(data_test1)\n",
    "#print('################# data_test1 after fixing')\n",
    "#print(data_test1)\n",
    "\n",
    "#   print('1',data_test1['corpus'])\n",
    "\n",
    "re_stop_words = remove_stop_words()\n",
    "data_test1['corpus'] = data_test1['corpus'].apply(removeStopWords, re_stop_words=re_stop_words)\n",
    "data = data_test1\n",
    "#   print('2',data_test1['corpus'])\n",
    "data_test1 = apply_stem(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test1.to_csv('./data/'+proj_name+'binaryNew.csv', encoding='utf-8', header=True, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Util',\n",
       " 'NLP',\n",
       " 'APM',\n",
       " 'Network',\n",
       " 'DB',\n",
       " 'Interpreter',\n",
       " 'Error Handling',\n",
       " 'Logging',\n",
       " 'Lang',\n",
       " 'Data Structure',\n",
       " 'DevOps',\n",
       " 'i18n',\n",
       " 'Setup',\n",
       " 'Logic',\n",
       " 'Microservices',\n",
       " 'ML',\n",
       " 'Test',\n",
       " 'Search',\n",
       " 'IO',\n",
       " 'UI',\n",
       " 'Parser',\n",
       " 'Security',\n",
       " 'Cloud',\n",
       " 'Big Data',\n",
       " 'Event Handling',\n",
       " 'App',\n",
       " 'GIS']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data.columns.values.tolist()\n",
    "labels = cols[2:len(cols)-1]\n",
    "labelsdf = pd.DataFrame(labels)\n",
    "labelsdf.to_csv('./data/'+proj_name+'labelsNew.csv', encoding='utf-8', header=False, index=False, sep=',')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "DATA_PATH = './data/'\n",
    "LABEL_PATH = './data/'\n",
    "from fast_bert.data_cls import BertDataBunch\n",
    "\n",
    "databunch = BertDataBunch(DATA_PATH, LABEL_PATH,\n",
    "                          tokenizer='bert-base-uncased',\n",
    "                          #train_file='train.csv',\n",
    "                          #val_file='val.csv',\n",
    "                          #label_file='labels.csv',\n",
    "                          train_file='binaryNew.csv',\n",
    "                          val_file='binaryNew.csv',\n",
    "                          label_file='labelsNew.csv',\n",
    "                          #text_col='text',\n",
    "                          text_col='corpus',\n",
    "                          label_col=labels,\n",
    "                          batch_size_per_gpu=16,\n",
    "                          max_seq_length=512,\n",
    "                          multi_gpu=False,\n",
    "                          multi_label=True,\n",
    "                          model_type='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import accuracy\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "#device_cuda = torch.device(\"cuda\")\n",
    "metrics = [{'name': 'accuracy', 'function': accuracy}]\n",
    "\n",
    "OUTPUT_DIR = './output/'\n",
    "\n",
    "learner = BertLearner.from_pretrained_model(\n",
    "\t\t\t\t\t\tdatabunch,\n",
    "\t\t\t\t\t\tpretrained_path='bert-base-uncased',\n",
    "\t\t\t\t\t\tmetrics=metrics,\n",
    "\t\t\t\t\t\t#device=device_cuda,\n",
    "                        device=None,\n",
    "\t\t\t\t\t\tlogger=logger,\n",
    "\t\t\t\t\t\toutput_dir=OUTPUT_DIR,\n",
    "\t\t\t\t\t\tfinetuned_wgts_path=None,\n",
    "\t\t\t\t\t\twarmup_steps=500,\n",
    "\t\t\t\t\t\tmulti_gpu=False,\n",
    "\t\t\t\t\t\tis_fp16=True,\n",
    "\t\t\t\t\t\tmulti_label=True,\n",
    "\t\t\t\t\t\tlogging_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(start_lr=1e-5,optimizer_type='lamb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(epochs=6,\n",
    "\t\t\tlr=6e-5,\n",
    "\t\t\tvalidate=True, \t# Evaluate the model after each epoch\n",
    "\t\t\tschedule_type=\"warmup_cosine\",\n",
    "\t\t\toptimizer_type=\"lamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
